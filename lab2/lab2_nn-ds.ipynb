{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 2: Neural Text Classification\n",
    "\n",
    "In this lab, we will practice building a neural network for text classification. \n",
    "\n",
    "The tutorial code is adopted from the official PyTorch tutorial: *Text classification with the torchtext library*\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#text-classification-with-the-torchtext-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install datasets\n",
    "\n",
    "Url: https://huggingface.co/docs/datasets/en/installation\n",
    "```bash\n",
    "conda install -c huggingface -c conda-forge datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "from data_utils import DatasetIterator, get_tokenizer, build_vocab_from_iter, to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\nlp-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Test datasets package, with SST2\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('glue', 'sst2') # it take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hide new secretions from the parental units ', 0)\n",
      "('contains no wit , only labored gags ', 0)\n",
      "('that loves its characters and communicates something rather beautiful about human nature ', 1)\n",
      "('remains utterly satisfied to remain the same throughout ', 0)\n",
      "('on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up ', 0)\n",
      "(\"that 's far too tragic to merit such superficial treatment \", 0)\n"
     ]
    }
   ],
   "source": [
    "# Check the raw data\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "\n",
    "count = 0\n",
    "for item in train_iter:\n",
    "    print(item)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']\n",
      "['contains', 'no', 'wit', ',', 'only', 'labored', 'gags']\n",
      "['that', 'loves', 'its', 'characters', 'and', 'communicates', 'something', 'rather', 'beautiful', 'about', 'human', 'nature']\n",
      "['remains', 'utterly', 'satisfied', 'to', 'remain', 'the', 'same', 'throughout']\n"
     ]
    }
   ],
   "source": [
    "# Check the output of yield_tokens()\n",
    "count = 0\n",
    "for tokens in yield_tokens(DatasetIterator(ds['train'])): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary\n",
    "\n",
    "First, we build the vocabulary using the `build_vocab_from_iterator` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iter(yield_tokens(DatasetIterator(ds['train'])), specials=[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all strings are converted into integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 16, 1587]\n",
      "[4460, 92, 12405, 38, 1, 7259, 8990]\n",
      "[5, 6549]\n",
      "[224, 10, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check the vocab\n",
    "print(vocab(['here', 'is', 'an', 'example']))\n",
    "print(vocab(['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']))\n",
    "print(vocab(['of', 'saucy']))\n",
    "\n",
    "# What about unknown words, i.e., out-of-vocabulary (OOV) words?\n",
    "print(vocab(['here', 'is', 'a', '@#$@!#$%']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, further define the `text_pipeline` and `label_pipeline` functions, for converting strings to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 16, 1587]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('here is an example')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Batchify Data \n",
    "\n",
    "Your goal is to define the `Collate_batch` function, which will be used to process the \"raw\" data batch.\n",
    "\n",
    "*Hint*: In the loop of `collate_batch`, the labels, tokens, and the offsets of all the examples are collected into three lists. Finally, the lists are converted into tensors. \n",
    "\n",
    "*Hint*: The `offsets` need to contain the cumulative positions of tokens in the batch. \n",
    "For example, if the batch contains 3 examples, whose lengths are `[1,3,2]`, then the final offsets should be `[0,1,4,6]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        token_ids = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0)) # Note that offsets contains the length (number of tokens) of each example\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the defined `collate_batch` function to create the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in this batch:  82\n",
      "Number of examples in one batch:  8\n",
      "Example 0:  tensor([ 4460,    92, 12405,    38,     1,  7259,  8990], device='cuda:0')\n",
      "Example 7:  tensor([   5, 6549], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens in this batch: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 0: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 7: ', token_ids[offsets[7]:])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# Number of tokens in this batch:  82\n",
    "# Number of examples in one batch:  8\n",
    "# Example 0:  tensor([ 4460,    92, 12405,    38,     1,  7259,  8990])\n",
    "# Example 7:  tensor([   5, 6549])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Define the Model\n",
    "\n",
    "The model consists of two parts of parameters: an embedding layer and a fully connected layer.\n",
    "\n",
    "Your need to first initialize an `nn.EmbeddingBag` instance and a `nn.Linear` instance:\n",
    "- The embedding layer should be initialized with `vocab_size`, `embed_dim`, and `sparse=False`.\n",
    "- The fully connected layer should have `embed_dim` as input size and `num_class` as output size.\n",
    "\n",
    "Then, in the `forward` function, the embedding layer should called with `token_ids` and `offsets` as inputs. The output of embedding layer is fed to the fully connected layer to get the final output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        ### END YOUR CODE ###\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        ### START YOUR CODE ###\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        out = self.fc(embedded)\n",
    "        ### END YOUR CODE ###\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "num_class = len(set([label for (_, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64 # embedding size\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "print('output size:', output.size())\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# output size: torch.Size([8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Define the loss function\n",
    "\n",
    "Cross entropy loss should be used. You can use `torch.nn.CrossEntropyLoss` to define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "### START YOUR CODE ###\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "### END YOUR CODE ###\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the loss function `criterion` works\n",
    "\n",
    "In the cell below, implement the **manual computation** of cross entropy loss. Use the formula $-y_i\\log(\\hat{y_i})$, \n",
    "\n",
    "where $y_i$ is the $i$-th ground truth label in `labels`, and $\\hat{y_i}$ is the predicted probability in `output` of the correponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([8, 2])\n",
      "loss: tensor(0.7222, device='cuda:0')\n",
      "loss_manual mean: tensor(0.7222, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "print('output shape:', output.shape)\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "# Manually calculate the loss\n",
    "loss_manual = []\n",
    "for i in range(output.shape[0]):\n",
    "    ### START YOUR CODE ###\n",
    "    probs = torch.softmax(output[i], dim=0)\n",
    "    l = -torch.log(probs[labels[i]])\n",
    "    ### END YOUR CODE ###\n",
    "    loss_manual.append(l)\n",
    "loss_manual = torch.stack(loss_manual)\n",
    "print('loss_manual mean:', loss_manual.mean())\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# output shape: torch.Size([8, 2])\n",
    "# loss: tensor(XXXXX)\n",
    "# loss_manual mean: tensor(XXXXX)\n",
    "# loss equals loss_manual: tensor(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Train and Evaluate Functions\n",
    "\n",
    "Define train and evaluate functions.\n",
    "\n",
    "You need to implement the forward pass, loss computation, backward propagation, and parameter update in the `train` function. \n",
    "\n",
    "Also, for each batch of data, calculate the total number of correctly predicted examples, by comparing `output` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        ### START YOUR CODE ###\n",
    "        # Forward pass\n",
    "        output = model(token_ids, offsets)\n",
    "        ### END YOUR CODE ###\n",
    "        try:\n",
    "            ### START YOUR CODE ###\n",
    "            # Compute loss\n",
    "            loss = criterion(output, labels)\n",
    "            ### END YOUR CODE ###\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            print('token_ids: ', token_ids)\n",
    "            print('offsets: ', offsets)\n",
    "            raise\n",
    "        ### START YOUR CODE ###\n",
    "        # Backward propagation, grad clipping, and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        # Calculate correct prediction in current batch\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        # Similar to the code in train function, but without backpropagation\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, label)\n",
    "        total_acc += (output.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train, valid, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = DatasetIterator(ds['train'])\n",
    "test_iter = DatasetIterator(ds['test'])\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 7998 batches | accuracy    0.571\n",
      "| epoch   1 |  1000/ 7998 batches | accuracy    0.661\n",
      "| epoch   1 |  1500/ 7998 batches | accuracy    0.712\n",
      "| epoch   1 |  2000/ 7998 batches | accuracy    0.750\n",
      "| epoch   1 |  2500/ 7998 batches | accuracy    0.773\n",
      "| epoch   1 |  3000/ 7998 batches | accuracy    0.771\n",
      "| epoch   1 |  3500/ 7998 batches | accuracy    0.797\n",
      "| epoch   1 |  4000/ 7998 batches | accuracy    0.807\n",
      "| epoch   1 |  4500/ 7998 batches | accuracy    0.799\n",
      "| epoch   1 |  5000/ 7998 batches | accuracy    0.804\n",
      "| epoch   1 |  5500/ 7998 batches | accuracy    0.819\n",
      "| epoch   1 |  6000/ 7998 batches | accuracy    0.824\n",
      "| epoch   1 |  6500/ 7998 batches | accuracy    0.838\n",
      "| epoch   1 |  7000/ 7998 batches | accuracy    0.833\n",
      "| epoch   1 |  7500/ 7998 batches | accuracy    0.848\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  9.92s | valid accuracy    0.835 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  1000/ 7998 batches | accuracy    0.879\n",
      "| epoch   2 |  1500/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  2000/ 7998 batches | accuracy    0.883\n",
      "| epoch   2 |  2500/ 7998 batches | accuracy    0.873\n",
      "| epoch   2 |  3000/ 7998 batches | accuracy    0.877\n",
      "| epoch   2 |  3500/ 7998 batches | accuracy    0.888\n",
      "| epoch   2 |  4000/ 7998 batches | accuracy    0.877\n",
      "| epoch   2 |  4500/ 7998 batches | accuracy    0.884\n",
      "| epoch   2 |  5000/ 7998 batches | accuracy    0.887\n",
      "| epoch   2 |  5500/ 7998 batches | accuracy    0.887\n",
      "| epoch   2 |  6000/ 7998 batches | accuracy    0.885\n",
      "| epoch   2 |  6500/ 7998 batches | accuracy    0.890\n",
      "| epoch   2 |  7000/ 7998 batches | accuracy    0.888\n",
      "| epoch   2 |  7500/ 7998 batches | accuracy    0.882\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  9.47s | valid accuracy    0.890 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 7998 batches | accuracy    0.908\n",
      "| epoch   3 |  1000/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  1500/ 7998 batches | accuracy    0.904\n",
      "| epoch   3 |  2000/ 7998 batches | accuracy    0.904\n",
      "| epoch   3 |  2500/ 7998 batches | accuracy    0.904\n",
      "| epoch   3 |  3000/ 7998 batches | accuracy    0.901\n",
      "| epoch   3 |  3500/ 7998 batches | accuracy    0.902\n",
      "| epoch   3 |  4000/ 7998 batches | accuracy    0.908\n",
      "| epoch   3 |  4500/ 7998 batches | accuracy    0.913\n",
      "| epoch   3 |  5000/ 7998 batches | accuracy    0.908\n",
      "| epoch   3 |  5500/ 7998 batches | accuracy    0.900\n",
      "| epoch   3 |  6000/ 7998 batches | accuracy    0.893\n",
      "| epoch   3 |  6500/ 7998 batches | accuracy    0.902\n",
      "| epoch   3 |  7000/ 7998 batches | accuracy    0.899\n",
      "| epoch   3 |  7500/ 7998 batches | accuracy    0.890\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 10.70s | valid accuracy    0.895 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 7998 batches | accuracy    0.920\n",
      "| epoch   4 |  1000/ 7998 batches | accuracy    0.921\n",
      "| epoch   4 |  1500/ 7998 batches | accuracy    0.920\n",
      "| epoch   4 |  2000/ 7998 batches | accuracy    0.917\n",
      "| epoch   4 |  2500/ 7998 batches | accuracy    0.921\n",
      "| epoch   4 |  3000/ 7998 batches | accuracy    0.911\n",
      "| epoch   4 |  3500/ 7998 batches | accuracy    0.912\n",
      "| epoch   4 |  4000/ 7998 batches | accuracy    0.912\n",
      "| epoch   4 |  4500/ 7998 batches | accuracy    0.904\n",
      "| epoch   4 |  5000/ 7998 batches | accuracy    0.909\n",
      "| epoch   4 |  5500/ 7998 batches | accuracy    0.919\n",
      "| epoch   4 |  6000/ 7998 batches | accuracy    0.913\n",
      "| epoch   4 |  6500/ 7998 batches | accuracy    0.910\n",
      "| epoch   4 |  7000/ 7998 batches | accuracy    0.913\n",
      "| epoch   4 |  7500/ 7998 batches | accuracy    0.913\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 10.87s | valid accuracy    0.895 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 7998 batches | accuracy    0.931\n",
      "| epoch   5 |  1000/ 7998 batches | accuracy    0.918\n",
      "| epoch   5 |  1500/ 7998 batches | accuracy    0.926\n",
      "| epoch   5 |  2000/ 7998 batches | accuracy    0.923\n",
      "| epoch   5 |  2500/ 7998 batches | accuracy    0.920\n",
      "| epoch   5 |  3000/ 7998 batches | accuracy    0.930\n",
      "| epoch   5 |  3500/ 7998 batches | accuracy    0.914\n",
      "| epoch   5 |  4000/ 7998 batches | accuracy    0.919\n",
      "| epoch   5 |  4500/ 7998 batches | accuracy    0.923\n",
      "| epoch   5 |  5000/ 7998 batches | accuracy    0.918\n",
      "| epoch   5 |  5500/ 7998 batches | accuracy    0.918\n",
      "| epoch   5 |  6000/ 7998 batches | accuracy    0.921\n",
      "| epoch   5 |  6500/ 7998 batches | accuracy    0.921\n",
      "| epoch   5 |  7000/ 7998 batches | accuracy    0.919\n",
      "| epoch   5 |  7500/ 7998 batches | accuracy    0.918\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  9.23s | valid accuracy    0.890 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 7998 batches | accuracy    0.939\n",
      "| epoch   6 |  1000/ 7998 batches | accuracy    0.949\n",
      "| epoch   6 |  1500/ 7998 batches | accuracy    0.947\n",
      "| epoch   6 |  2000/ 7998 batches | accuracy    0.942\n",
      "| epoch   6 |  2500/ 7998 batches | accuracy    0.937\n",
      "| epoch   6 |  3000/ 7998 batches | accuracy    0.946\n",
      "| epoch   6 |  3500/ 7998 batches | accuracy    0.948\n",
      "| epoch   6 |  4000/ 7998 batches | accuracy    0.947\n",
      "| epoch   6 |  4500/ 7998 batches | accuracy    0.946\n",
      "| epoch   6 |  5000/ 7998 batches | accuracy    0.946\n",
      "| epoch   6 |  5500/ 7998 batches | accuracy    0.945\n",
      "| epoch   6 |  6000/ 7998 batches | accuracy    0.944\n",
      "| epoch   6 |  6500/ 7998 batches | accuracy    0.945\n",
      "| epoch   6 |  7000/ 7998 batches | accuracy    0.947\n",
      "| epoch   6 |  7500/ 7998 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 10.04s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 7998 batches | accuracy    0.953\n",
      "| epoch   7 |  1000/ 7998 batches | accuracy    0.950\n",
      "| epoch   7 |  1500/ 7998 batches | accuracy    0.944\n",
      "| epoch   7 |  2000/ 7998 batches | accuracy    0.954\n",
      "| epoch   7 |  2500/ 7998 batches | accuracy    0.950\n",
      "| epoch   7 |  3000/ 7998 batches | accuracy    0.950\n",
      "| epoch   7 |  3500/ 7998 batches | accuracy    0.944\n",
      "| epoch   7 |  4000/ 7998 batches | accuracy    0.949\n",
      "| epoch   7 |  4500/ 7998 batches | accuracy    0.949\n",
      "| epoch   7 |  5000/ 7998 batches | accuracy    0.949\n",
      "| epoch   7 |  5500/ 7998 batches | accuracy    0.953\n",
      "| epoch   7 |  6000/ 7998 batches | accuracy    0.942\n",
      "| epoch   7 |  6500/ 7998 batches | accuracy    0.945\n",
      "| epoch   7 |  7000/ 7998 batches | accuracy    0.944\n",
      "| epoch   7 |  7500/ 7998 batches | accuracy    0.942\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  8.96s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 7998 batches | accuracy    0.953\n",
      "| epoch   8 |  1000/ 7998 batches | accuracy    0.955\n",
      "| epoch   8 |  1500/ 7998 batches | accuracy    0.948\n",
      "| epoch   8 |  2000/ 7998 batches | accuracy    0.954\n",
      "| epoch   8 |  2500/ 7998 batches | accuracy    0.946\n",
      "| epoch   8 |  3000/ 7998 batches | accuracy    0.950\n",
      "| epoch   8 |  3500/ 7998 batches | accuracy    0.954\n",
      "| epoch   8 |  4000/ 7998 batches | accuracy    0.951\n",
      "| epoch   8 |  4500/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  5000/ 7998 batches | accuracy    0.954\n",
      "| epoch   8 |  5500/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  6000/ 7998 batches | accuracy    0.944\n",
      "| epoch   8 |  6500/ 7998 batches | accuracy    0.953\n",
      "| epoch   8 |  7000/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  7500/ 7998 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 10.05s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 7998 batches | accuracy    0.955\n",
      "| epoch   9 |  1000/ 7998 batches | accuracy    0.952\n",
      "| epoch   9 |  1500/ 7998 batches | accuracy    0.954\n",
      "| epoch   9 |  2000/ 7998 batches | accuracy    0.945\n",
      "| epoch   9 |  2500/ 7998 batches | accuracy    0.949\n",
      "| epoch   9 |  3000/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  3500/ 7998 batches | accuracy    0.951\n",
      "| epoch   9 |  4000/ 7998 batches | accuracy    0.952\n",
      "| epoch   9 |  4500/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  5000/ 7998 batches | accuracy    0.952\n",
      "| epoch   9 |  5500/ 7998 batches | accuracy    0.954\n",
      "| epoch   9 |  6000/ 7998 batches | accuracy    0.951\n",
      "| epoch   9 |  6500/ 7998 batches | accuracy    0.952\n",
      "| epoch   9 |  7000/ 7998 batches | accuracy    0.951\n",
      "| epoch   9 |  7500/ 7998 batches | accuracy    0.953\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 10.69s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  1000/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  1500/ 7998 batches | accuracy    0.946\n",
      "| epoch  10 |  2000/ 7998 batches | accuracy    0.949\n",
      "| epoch  10 |  2500/ 7998 batches | accuracy    0.959\n",
      "| epoch  10 |  3000/ 7998 batches | accuracy    0.946\n",
      "| epoch  10 |  3500/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  4000/ 7998 batches | accuracy    0.950\n",
      "| epoch  10 |  4500/ 7998 batches | accuracy    0.953\n",
      "| epoch  10 |  5000/ 7998 batches | accuracy    0.946\n",
      "| epoch  10 |  5500/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  6000/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  6500/ 7998 batches | accuracy    0.945\n",
      "| epoch  10 |  7000/ 7998 batches | accuracy    0.950\n",
      "| epoch  10 |  7500/ 7998 batches | accuracy    0.958\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  9.40s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Test Data\n",
    "\n",
    "This is a necessary step. But since the `test` split of SST2 is not annotated, we will use the `dev` split here to pretend it is the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.911\n"
     ]
    }
   ],
   "source": [
    "accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "\n",
    "# Your test accuracy should be around 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Test the model with a few unannotated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a positive sentiment.\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = ['negative', 'positive']\n",
    "\n",
    "def predict(text, model, vocab, tokenizer, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(vocab(tokenizer(text)), device=device)\n",
    "        output = model(text, torch.tensor([0], device=device))\n",
    "        return labels[output.argmax(1).item()]\n",
    "\n",
    "ex_text_str = \"A very well-made, funny and entertaining picture.\"\n",
    "print(\"This is a %s sentiment.\" % (predict(ex_text_str, model, vocab, tokenizer, sentiment_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully built and trained a neural network model to classify sentiment in text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

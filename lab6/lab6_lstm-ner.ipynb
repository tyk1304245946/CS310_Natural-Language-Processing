{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 6: LSTM for Named Entity Recognition (NER)\n",
    "\n",
    "In this lab, we practice the data and model preparation for using LSTM for the NER task. \n",
    "\n",
    "The dataset is CoNLL2003 English named entity recognition (NER). The dataset is a collection of news articles from Reuters. \n",
    "\n",
    "The dataset is annotated with four types of named entities: \n",
    "`[persons, locations, organizations, miscellaneous]`. (`miscellaneous` does not belong to the previous three types)\n",
    "\n",
    "The dataset is divided into three parts: **training**, **development**, and **testing**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from utils import Indexer, read_ner_data_from_connl, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/dev.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "EMBEDDINGS_PATH = 'data/glove.6B.100d.txt' \n",
    "# Download from https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# It includes dimension 50, 100, 200, and 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is in the IOB format. \n",
    "The IOB format is a simple text chunking format that divides the text into chunks and assigns a label to each chunk. \n",
    "\n",
    "The label is a combination of two parts: \n",
    "- the type of the named entity\n",
    "- the position of the word in the named entity. \n",
    "\n",
    "The type of the named entity is one of the four types `[persons, locations, organizations, miscellaneous]`. \n",
    "\n",
    "The position of the word in the named entity is one of three positions: `B` (beginning), `I` (inside), and `O` (outside). \n",
    "\n",
    "Examples:\n",
    "- \"New\" in the named entity \"New York\" is labeled as \"B-LOC\", and \"York\" is labeled as \"I-LOC\". \n",
    "- The word \"I\" in the sentence \"I live in New York\" is labeled as \"O\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_ner_data_from_connl(TRAIN_PATH)\n",
    "dev_words, dev_tags = read_ner_data_from_connl(DEV_PATH)\n",
    "test_words, test_tags = read_ner_data_from_connl(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train words size: 203621\n",
      "dev words size: 51362\n",
      "test words size: 46435\n"
     ]
    }
   ],
   "source": [
    "print('train words size:', len(train_words))\n",
    "print('dev words size:', len(dev_words))\n",
    "print('test words size:', len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', 'Peter']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "print(train_words[:10])\n",
    "print(train_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EU', 'B-ORG'),\n",
      " ('rejects', 'O'),\n",
      " ('German', 'B-MISC'),\n",
      " ('call', 'O'),\n",
      " ('to', 'O'),\n",
      " ('boycott', 'O'),\n",
      " ('British', 'B-MISC'),\n",
      " ('lamb', 'O'),\n",
      " ('.', 'O'),\n",
      " ('Peter', 'B-PER')]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(train_words[:10], train_tags[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that each sentence ends with token '.' and tag 'O'. Between sentences there is a blank line.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Build vocabularies for both words and tags\n",
    "\n",
    "\n",
    "`utils.py` provides an `Indexer` class that can be used to convert words and tags to indices and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of word \"the\": 40\n",
      "index of tag \"O\": 2\n",
      "word with index 0: <UNKNOWN>\n",
      "word with index 100 Fischler\n",
      "tag with index 0: <UNKNOWN>\n",
      "tag with index 1: B-ORG\n"
     ]
    }
   ],
   "source": [
    "indexer_train_words = Indexer(train_words)\n",
    "indexer_train_tags = Indexer(train_tags)\n",
    "\n",
    "# Test\n",
    "print('index of word \"the\":', indexer_train_words.element_to_index('the'))\n",
    "print('index of tag \"O\":', indexer_train_tags.element_to_index('O'))\n",
    "print('word with index 0:', indexer_train_words.index_to_element(0))\n",
    "print('word with index 100', indexer_train_words.index_to_element(100))\n",
    "print('tag with index 0:', indexer_train_tags.index_to_element(0))\n",
    "print('tag with index 1:', indexer_train_tags.index_to_element(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since the train, test, and dev sets are different, we need to build the vocabularies using **ALL** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes of indexers from all data:\n",
      "30290 10\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "indexer_words = Indexer(train_words + dev_words + test_words)\n",
    "indexer_tags = Indexer(train_tags + dev_tags + test_tags)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('sizes of indexers from all data:')\n",
    "print(len(indexer_words), len(indexer_tags))\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# sizes of indexers from all data:\n",
    "# 30290 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Prepare data in batch\n",
    "\n",
    "What it means by a \"batch\" of data is different from Lab 5 (Language Modeling).\n",
    "\n",
    "Because the sequence boundaries are some-what difficult to determine, and the sequences are of varying lengths, for this NER lab, we use a sloppy way to create batches: Simply use a fixed size (`batch_size`) of tokens as a batch. So there is just one long sequence in each batch.\n",
    "\n",
    "`utils.py` provides a `get_batch` function that yields `(words, tags)` in specified batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches[0] sizes: 128 128\n",
      "batches[1] sizes: 128 128\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "batches = list(get_batch(train_words, train_tags, batch_size))\n",
    "\n",
    "# Test\n",
    "print('batches[0] sizes:', len(batches[0][0]), len(batches[0][1])) \n",
    "print('batches[1] sizes:', len(batches[1][0]), len(batches[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Indexer.elements_to_indices` to convert words and tags to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_ids[:10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "tags_ids[:10] [1, 2, 3, 2, 2, 2, 3, 2, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "sequence, tags = batches[0]\n",
    "\n",
    "### START YOUR CODE ###\n",
    "sequence_ids = [indexer_words.element_to_index(word) for word in sequence]\n",
    "tags_ids = [indexer_tags.element_to_index(tag) for tag in tags]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('sequence_ids[:10]', sequence_ids[:10])\n",
    "print('tags_ids[:10]', tags_ids[:10])\n",
    "\n",
    "# You are expected to see the following output:\n",
    "# sequence_ids[:10] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# tags_ids[:10] [1, 2, 3, 2, 2, 2, 3, 2, 2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Practice LSTM module\n",
    "\n",
    "Create a LSTM unit that takes input of dimension 3 and produces hidden state of dimension 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([1, 4])\n",
      "hidden hn size: torch.Size([1, 4])\n",
      "hidden cn size: torch.Size([1, 4])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 4)\n",
    "\n",
    "# Make a sequence of length 5\n",
    "input_seq = [torch.randn(1, 3) for _ in range(5)]\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h0 = torch.randn(1, 4)\n",
    "c0 = torch.randn(1, 4)\n",
    "hidden = (h0, c0)\n",
    "\n",
    "# Run forward pass with a loop\n",
    "for input_t in input_seq:\n",
    "    out, hidden = lstm(input_t, hidden) # Note that the hidden state from the previous time step is used as input for the current time step\n",
    "\n",
    "# Test output\n",
    "print('output size:', out.size())\n",
    "print('hidden hn size:', hidden[0].size())\n",
    "print('hidden cn size:', hidden[1].size())\n",
    "print(torch.equal(out, hidden[0])) # out is just the last hidden state hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same forward pass can be done with a single call to `lstm`, providing the entire sequence at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([5, 4])\n",
      "hidden hn size: torch.Size([1, 4])\n",
      "hidden cn size: torch.Size([1, 4])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Make a sequence of length 5 in a single tensor\n",
    "input_seq2 = torch.cat(input_seq, dim=0)\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h0 = torch.randn(1, 4)\n",
    "c0 = torch.randn(1, 4)\n",
    "hidden = (h0, c0)\n",
    "\n",
    "# Run forward pass with a single call\n",
    "out, hidden = lstm(input_seq2, hidden)\n",
    "\n",
    "# Test output\n",
    "print('output size:', out.size())\n",
    "print('hidden hn size:', hidden[0].size())\n",
    "print('hidden cn size:', hidden[1].size())\n",
    "\n",
    "print(torch.equal(out, hidden[0])) # this time out != hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time `out` is a sequence of hidden states for all times steps, not just the last one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a bi-directional LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([5, 8])\n",
      "hidden hn size: torch.Size([2, 4])\n",
      "hidden cn size: torch.Size([2, 4])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "bilstm = nn.LSTM(3, 4, bidirectional=True)\n",
    "\n",
    "# Initialize hidden state and cell state\n",
    "h0 = torch.randn(2, 4)\n",
    "c0 = torch.randn(2, 4)\n",
    "hidden = (h0, c0)\n",
    "\n",
    "# Forward pass\n",
    "out, hidden = bilstm(input_seq2, hidden)\n",
    "\n",
    "# Test output\n",
    "print('output size:', out.size())\n",
    "print('hidden hn size:', hidden[0].size())\n",
    "print('hidden cn size:', hidden[1].size())\n",
    "\n",
    "print(torch.equal(out, hidden[0])) # this time out != hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output size becomes $2\\times4=8$ because the LSTM is bidirectional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4. Using LSTM for tagging\n",
    "\n",
    "Here we practice using LSTM for tagging tasks (e.g., POS, NER). \n",
    "We will not use advanced architecture like maximum entropy Markov model (MEMM), or advanced decoding strategies such as Viterbi, or beam search decoding.\n",
    "\n",
    "The model is as follows: let the input sentence be\n",
    "$w_1, \\dots, w_M$, where $w_i \\in V$, our vocab. Also, let\n",
    "$T$ be our tag set, and $y_i$ the tag of word $w_i$.\n",
    "\n",
    "\n",
    "Denote our prediction of the tag of word $w_i$ by\n",
    "$\\hat{y}_i$.\n",
    "This is a structure prediction, model, where our output is a sequence\n",
    "$\\hat{y}_1, \\dots, \\hat{y}_M$, where $\\hat{y}_i \\in T$.\n",
    "\n",
    "To do the prediction, pass an LSTM over the sentence. Denote the hidden\n",
    "state at timestep $i$ as $h_i$. Also, assign each tag a\n",
    "unique index. \n",
    "\n",
    "Then our prediction rule for $\\hat{y}_i$ is\n",
    "\n",
    "\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(U h_i + b))_j\\end{align}\n",
    "\n",
    "That is, take the log softmax of the transformation of the hidden state $h_i$,\n",
    "and the predicted tag is the tag that has the maximum log probability. \n",
    "\n",
    "Parameters $U$ and $b$ can be implemented as a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "input_tags: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "VOCAB_SIZE = len(indexer_words)\n",
    "TAGSET_SIZE = len(indexer_tags)\n",
    "\n",
    "input_sequence = train_words[:9]\n",
    "input_tags = train_tags[:9]\n",
    "\n",
    "print('input_sequence:', input_sequence)\n",
    "print('input_tags:', input_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the model.\n",
    "\n",
    "In `__init__` method, initialize `word_embeddings` with a pretrained embedding weight matrix loaded from `glove.6B.100d.txt`.\n",
    "\n",
    "For some advanced variants of model, e.g., maximum entropy Markov model (MEMM), you also need to initialize `tag_embeddings` with a random weight matrix.\n",
    "\n",
    "`forward` method takes the sequence of word indices as input and returns the log probabilities of predicted tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        # self.word_embeddings = None # nn.Embedding\n",
    "        # self.lstm = None # Define LSTM\n",
    "        # self.fc = None # Define linear layer\n",
    "        self.word_emmbeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        ### START YOUR CODE ###\n",
    "        # sequence = torch.tensor(sequence)\n",
    "        embeds = self.word_emmbeddings(sequence) # Embed input sequence\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sequence), 1, -1)) # LSTM forward pass, return hidden state\n",
    "        logits = self.fc(lstm_out.view(len(sequence), -1)) # Linear layer forward pass, return logits\n",
    "        logprobs = F.log_softmax(logits, dim=1) # Log softmax\n",
    "        ### END YOUR CODE ###\n",
    "        return logprobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and test the forward computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs shape: torch.Size([9, 10])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### START YOUR CODE ###\n",
    "    # inputs_tensor = None # Convert input sequence to tensor, using indexer_words\n",
    "    # logprobs = None\n",
    "    inputs_tensor = torch.tensor([indexer_words.element_to_index(word) for word in input_sequence])\n",
    "    logprobs = model(inputs_tensor)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test output\n",
    "print('logprobs shape:', logprobs.shape)\n",
    "# You are expected to see the following:\n",
    "# logprobs shape: torch.Size([9, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5. Evaluation\n",
    "\n",
    "Evaluation on multiple metrics are needed. Here we practice using the provided `metrices.py` file as a helper. \n",
    "\n",
    "In `metrices.py` there is a `MetricsHandler` class, which has an `update` method that should be called for every batch during training. \n",
    "It also has a `collect` method that should be called after each epoch.  \n",
    "\n",
    "It takes a list of classes (target tags) as input, so we need to specify this arguement properly with the `indexer_tags` object or `TAGSET_SIZE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import MetricsHandler\n",
    "\n",
    "train_metrics = MetricsHandler(classes=list(range(TAGSET_SIZE)))\n",
    "val_metrics = MetricsHandler(classes=list(range(TAGSET_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an sample segment of training and evaluate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "def train_loop():\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        for sequence, tags in batches:\n",
    "            sequence_ids = torch.tensor([indexer_words.element_to_index(word) for word in sequence])\n",
    "            tags_ids = torch.tensor([indexer_tags.element_to_index(tag) for tag in tags])\n",
    "            predictions = model(sequence_ids)\n",
    "            train_metrics.update(sequence_ids, tags_ids) # update() method takes the predictions and the ground truth tags as inputs\n",
    "\n",
    "        train_metrics.collect()\n",
    "\n",
    "        # print training metrics\n",
    "        for metric in train_metrics.metrics_dict.keys():\n",
    "                print(f\"{metric} - {train_metrics.metrics_dict[metric][-1]}\")\n",
    "        print()\n",
    "\n",
    "def evaluate_loop():\n",
    "    \"\"\"\n",
    "    Evaluation loop\n",
    "    \"\"\"\n",
    "    val_batches = get_batch(dev_words, dev_tags)\n",
    "    for sequence, tags in val_batches:\n",
    "        # make prediction\n",
    "        sequence_ids = torch.tensor([indexer_words.element_to_index(word) for word in sequence])\n",
    "        tags_ids = torch.tensor([indexer_tags.element_to_index(tag) for tag in tags])\n",
    "        predictions = model(sequence_ids)\n",
    "        val_metrics.update(sequence_ids, tags_ids)\n",
    "\n",
    "    val_metrics.collect()\n",
    "\n",
    "    # print validation metrics\n",
    "    for metric in val_metrics.metrics_dict.keys():\n",
    "        print(f\"{metric} - {val_metrics.metrics_dict[metric][-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 5\n",
    "\n",
    "# def train_loop():\n",
    "#     \"\"\"\n",
    "#     Training loop\n",
    "#     \"\"\"\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for sequence, tags in batches:\n",
    "#             # make prediction\n",
    "#             predictions = model(sequence)\n",
    "#             train_metrics.update(predictions, tags) # update() method takes the predictions and the ground truth tags as inputs\n",
    "\n",
    "#         train_metrics.collect()\n",
    "\n",
    "#         # print training metrics\n",
    "#         for metric in train_metrics.metrics_dict.keys():\n",
    "#                 print(f\"{metric} - {train_metrics.metrics_dict[metric][-1]}\")\n",
    "#         print()\n",
    "\n",
    "# def evaluate_loop():\n",
    "#     \"\"\"\n",
    "#     Evaluation loop\n",
    "#     \"\"\"\n",
    "#     val_batches = get_batch(dev_words, dev_tags)\n",
    "#     for sequence, tags in val_batches:\n",
    "#         # make prediction\n",
    "#         predictions = model(sequence)\n",
    "#         val_metrics.update(predictions, tags)\n",
    "\n",
    "#     val_metrics.collect()\n",
    "\n",
    "#     # print validation metrics\n",
    "#     for metric in val_metrics.metrics_dict.keys():\n",
    "#         print(f\"{metric} - {val_metrics.metrics_dict[metric][-1]}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyk13\\OneDrive\\SUSTech_study\\2025 Spring\\CS310_Natural-Language-Processing\\lab6\\metrics.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return x[1, 1]/(x[1, 1] + x[0, 1])\n",
      "c:\\Users\\tyk13\\OneDrive\\SUSTech_study\\2025 Spring\\CS310_Natural-Language-Processing\\lab6\\metrics.py:9: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return x[1, 1] / (x[1, 0] + x[1, 1])\n",
      "c:\\Users\\tyk13\\OneDrive\\SUSTech_study\\2025 Spring\\CS310_Natural-Language-Processing\\lab6\\metrics.py:15: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return ((1 + beta**2)*precision*recall)/(beta**2 * precision + recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - 0.33328115874701164\n",
      "Recall - 0.0035514188545347544\n",
      "F1-score - 0.011345584898791648\n",
      "F0.5-score - 0.026355780043801968\n",
      "\n",
      "Precision - 0.33328115874701164\n",
      "Recall - 0.0035514188545347544\n",
      "F1-score - 0.011345584898791648\n",
      "F0.5-score - 0.026355780043801968\n",
      "\n",
      "Precision - 0.33328115874701164\n",
      "Recall - 0.0035514188545347544\n",
      "F1-score - 0.011345584898791648\n",
      "F0.5-score - 0.026355780043801968\n",
      "\n",
      "Precision - 0.33328115874701164\n",
      "Recall - 0.0035514188545347544\n",
      "F1-score - 0.011345584898791648\n",
      "F0.5-score - 0.026355780043801968\n",
      "\n",
      "Precision - 0.33328115874701164\n",
      "Recall - 0.0035514188545347544\n",
      "F1-score - 0.011345584898791648\n",
      "F0.5-score - 0.026355780043801968\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_batch() missing 1 required positional argument: 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_loop()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mevaluate_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[135], line 25\u001b[0m, in \u001b[0;36mevaluate_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_loop\u001b[39m():\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Evaluation loop\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     val_batches \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_tags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequence, tags \u001b[38;5;129;01min\u001b[39;00m val_batches:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m# make prediction\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         sequence_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([indexer_words\u001b[38;5;241m.\u001b[39melement_to_index(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sequence])\n",
      "\u001b[1;31mTypeError\u001b[0m: get_batch() missing 1 required positional argument: 'size'"
     ]
    }
   ],
   "source": [
    "train_loop()\n",
    "\n",
    "evaluate_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
